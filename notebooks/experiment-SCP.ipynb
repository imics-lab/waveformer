{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgBrFqJNgMNT",
        "outputId": "0a0ead89-883c-4831-b929-9d0712fce757"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading SelfRegulationSCP2 from https://timeseriesclassification.com/aeon-toolkit/SelfRegulationSCP2.zip...\n",
            "Extracting SelfRegulationSCP2...\n",
            "Dataset SelfRegulationSCP2 extracted to datasets/SelfRegulationSCP2.\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension1_TRAIN.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension2_TRAIN.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension3_TRAIN.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension4_TRAIN.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension5_TRAIN.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension6_TRAIN.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension7_TRAIN.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension1_TEST.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension2_TEST.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension3_TEST.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension4_TEST.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension5_TEST.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension6_TEST.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP2/SelfRegulationSCP2Dimension7_TEST.arff\n",
            "X_train shape: torch.Size([200, 1152, 6]), y_train shape: torch.Size([200])\n",
            "X_valid shape: torch.Size([90, 1152, 6]), y_valid shape: torch.Size([90])\n",
            "X_test shape: torch.Size([90, 1152, 6]), y_test shape: torch.Size([90])\n",
            "Number of classes: 2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.io import arff\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Directory where datasets will be downloaded and extracted\n",
        "DATA_DIR = 'datasets'\n",
        "\n",
        "# Ensure the dataset directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def download_dataset(dataset_name, url):\n",
        "    \"\"\"\n",
        "    Downloads and extracts a zip file containing the dataset.\n",
        "    \"\"\"\n",
        "    zip_path = os.path.join(DATA_DIR, f\"{dataset_name}.zip\")\n",
        "    extract_path = os.path.join(DATA_DIR, dataset_name)\n",
        "\n",
        "    # Download the dataset\n",
        "    print(f\"Downloading {dataset_name} from {url}...\")\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "    # Extract the zip file\n",
        "    print(f\"Extracting {dataset_name}...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "    # Remove the zip file after extraction\n",
        "    os.remove(zip_path)\n",
        "    print(f\"Dataset {dataset_name} extracted to {extract_path}.\")\n",
        "    return extract_path\n",
        "\n",
        "def load_arff_data(file_path):\n",
        "    \"\"\"\n",
        "    Loads ARFF file and converts it to a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    print(f\"Loading ARFF file: {file_path}\")\n",
        "    data, meta = arff.loadarff(file_path)\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "def preprocess_data(train_paths, test_paths, batch_size=64):\n",
        "    \"\"\"\n",
        "    Preprocesses the SelfRegulationSCP1 data:\n",
        "    - Loads and combines multiple dimensions from ARFF files.\n",
        "    - Normalizes the features for each dimension.\n",
        "    - Stacks features from different dimensions.\n",
        "    - Converts them into PyTorch tensors.\n",
        "    - Creates DataLoaders for training, validation, and testing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load all training and test dimensions\n",
        "    train_dfs = [load_arff_data(path) for path in train_paths]\n",
        "    test_dfs = [load_arff_data(path) for path in test_paths]\n",
        "\n",
        "    # Separate features and labels for all dimensions\n",
        "    train_features = [df.drop(columns=['cortical']) for df in train_dfs]\n",
        "    test_features = [df.drop(columns=['cortical']) for df in test_dfs]\n",
        "\n",
        "    # Create a label mapping for the two unique class labels\n",
        "    label_mapping = {\n",
        "        b'negativity': 0,\n",
        "        b'positivity': 1\n",
        "    }\n",
        "\n",
        "    # Apply the label mapping to the training and test sets\n",
        "    train_labels = train_dfs[0]['cortical'].apply(lambda x: label_mapping[x]).values\n",
        "    test_labels = test_dfs[0]['cortical'].apply(lambda x: label_mapping[x]).values\n",
        "\n",
        "    # Normalize the features using StandardScaler for each dimension\n",
        "    scalers = [StandardScaler() for _ in range(6)]  # 6 dimensions\n",
        "    train_features_normalized = [scalers[i].fit_transform(train_features[i]) for i in range(6)]\n",
        "    test_features_normalized = [scalers[i].transform(test_features[i]) for i in range(6)]\n",
        "\n",
        "    # Stack all dimensions along a new axis (multivariate time-series)\n",
        "    X_train = np.stack(train_features_normalized, axis=-1)\n",
        "    X_test_full = np.stack(test_features_normalized, axis=-1)\n",
        "\n",
        "    # Split the test data into validation and test sets\n",
        "    X_valid, X_test, y_valid, y_test = train_test_split(X_test_full, test_labels, test_size=0.50, random_state=42)\n",
        "    y_train = train_labels\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.int64)\n",
        "\n",
        "    X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
        "    y_valid = torch.tensor(y_valid, dtype=torch.int64)\n",
        "\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.int64)\n",
        "\n",
        "    # Output dataset shapes\n",
        "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "    print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "    # Return both the DataLoaders and the raw tensors\n",
        "    return train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test\n",
        "\n",
        "# Example usage for downloading, extracting, and preprocessing the SelfRegulationSCP1 dataset\n",
        "if __name__ == \"__main__\":\n",
        "    # URL for the dataset\n",
        "    dataset_name = 'SelfRegulationSCP2'\n",
        "    dataset_url = 'https://timeseriesclassification.com/aeon-toolkit/SelfRegulationSCP2.zip'\n",
        "\n",
        "    # Download and extract the dataset\n",
        "    extract_path = download_dataset(dataset_name, dataset_url)\n",
        "\n",
        "    # Paths for the ARFF files\n",
        "    train_arff_paths = [\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension1_TRAIN.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension2_TRAIN.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension3_TRAIN.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension4_TRAIN.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension5_TRAIN.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension6_TRAIN.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension7_TRAIN.arff')\n",
        "    ]\n",
        "\n",
        "    test_arff_paths = [\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension1_TEST.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension2_TEST.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension3_TEST.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension4_TEST.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension5_TEST.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension6_TEST.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP2Dimension7_TEST.arff')\n",
        "    ]\n",
        "\n",
        "    # Preprocess the data\n",
        "    train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test = preprocess_data(train_arff_paths, test_arff_paths)\n",
        "\n",
        "    n_classes = len(torch.unique(y_train))\n",
        "\n",
        "    # Output the number of classes\n",
        "    print(f\"Number of classes: {n_classes}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caTlLTBNgNMM",
        "outputId": "70e1514b-7164-487c-abd0-5824bd64a3c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "WaveFormer Training (with Bucketing RPE)\n",
            "======================================================================\n",
            "Input timesteps: 1152\n",
            "Input channels: 6\n",
            "Num classes: 2\n",
            "Patch size: 128\n",
            "Embedding dim: 32\n",
            "Model parameters: 56,866\n",
            "======================================================================\n",
            "Epoch  10: Train=0.6750, Val=0.5556\n",
            "Epoch  20: Train=0.7800, Val=0.5222\n",
            "Epoch  30: Train=0.8500, Val=0.5778\n",
            "Epoch  40: Train=0.8300, Val=0.5222\n",
            "Epoch  50: Train=0.8700, Val=0.5444\n",
            "Epoch  60: Train=0.8850, Val=0.5111\n",
            "Epoch  70: Train=0.9100, Val=0.4889\n",
            "Epoch  80: Train=0.9450, Val=0.4667\n",
            "Epoch  90: Train=0.9600, Val=0.5111\n",
            "Epoch 100: Train=0.9550, Val=0.5111\n",
            "Early stopping at epoch 100\n",
            "\n",
            "======================================================================\n",
            "RESULTS:\n",
            "  Best Val Acc:  0.5778\n",
            "  Test Acc:      0.6222\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# from waveformer_final import train_waveformer\n",
        "\n",
        "model, val_acc, test_acc = train_waveformer(\n",
        "    X_train, y_train,\n",
        "    X_valid, y_valid,\n",
        "    X_test, y_test,\n",
        "    patch_size=128,\n",
        "    embedding_dim=32,\n",
        "    num_transformer_layers=2,\n",
        "    num_heads=2,\n",
        "    dim_feedforward=128,\n",
        "    dropout=0.1,\n",
        "    batch_size=8,\n",
        "    lr=1e-4,\n",
        "    num_epochs=200,\n",
        "    patience=70,\n",
        "    device='cpu'\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
